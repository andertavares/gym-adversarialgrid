{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(agent, env, episodes=20, episode_duration=100, render=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs a reinforcement learning experiment\n",
    "    :param agent: implements the method act(observation)\n",
    "    :param env: an instance of gym.Env\n",
    "    :param episodes: number of episodes to run\n",
    "    :param episode_duration: number of steps of each episode\n",
    "    :param render: render each step?\n",
    "    :param verbose: print transition information?\n",
    "    \"\"\"\n",
    "    for ep in range(episodes):\n",
    "        observation = env.reset()\n",
    "        acc_reward = 0\n",
    "        if render:\n",
    "            env.render() # show initial state\n",
    "        for t in range(100):         \n",
    "            # print(observation)\n",
    "            action = agent.act(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            acc_reward += reward\n",
    "            if verbose:\n",
    "                print(observation, reward, done, info)          \n",
    "            if render:\n",
    "                env.render() \n",
    "                \n",
    "            if done:\n",
    "                print(\"Episode {} finished after {} timesteps w/ total reward {}\".format(ep+1, t+1, acc_reward))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test a random agent\n",
    "import gym\n",
    "import gym_adversarialgrid.envs.adversarialgrid as adversarialgrid\n",
    "import gym_adversarialgrid.agents.adversary as agent\n",
    "#from gym_adversarialgrid.envs.adversarialgrid import AdversarialGrid\n",
    "\n",
    "\n",
    "env = adversarialgrid.AdversarialGrid(opponent='Fixed', action=adversarialgrid.NOOP, map='3x4')\n",
    "\n",
    "agent = agent.Random(env.observation_space, env.action_space)\n",
    "\n",
    "run(agent, env, render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# testing the tabular q agent, vs fixed NOOP adversary\n",
    "from pprint import pprint\n",
    "import gym_adversarialgrid.agents.tabular as tabular\n",
    "\n",
    "env = adversarialgrid.AdversarialGrid(opponent='Fixed', action=adversarialgrid.NOOP, map='3x4')\n",
    "agent = tabular.TabularQAgent(env.observation_space, env.action_space, eps=0.1, init_mean=1)\n",
    "\n",
    "#train\n",
    "agent.train(env, 10000)\n",
    "#pprint(agent.q)\n",
    "\n",
    "#test\n",
    "agent.config['eps'] = 0 #all greedy o/\n",
    "run(agent, env, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tabular q vs fixed deflector\n",
    "env = adversarialgrid.AdversarialGrid(opponent='Fixed', action=adversarialgrid.DEFLECT, map='3x4')\n",
    "agent = tabular.TabularQAgent(env.observation_space, env.action_space, eps=0.1, init_mean=1)\n",
    "\n",
    "#train\n",
    "agent.train(env, 10000)\n",
    "#pprint(agent.q)\n",
    "\n",
    "#test\n",
    "agent.config['eps'] = 0 #all greedy o/\n",
    "#run(agent, env, render=True, verbose=True)\n",
    "run(agent, env, render=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tabular q vs random\n",
    "import gym_adversarialgrid.envs.adversarialgrid as adversarialgrid\n",
    "import gym_adversarialgrid.agents.tabular as tabular\n",
    "\n",
    "env = adversarialgrid.AdversarialGrid(opponent='Random', map='3x4')\n",
    "agent = tabular.TabularQAgent(env.observation_space, env.action_space, eps=0.1, init_mean=1)\n",
    "\n",
    "#train\n",
    "agent.train(env, 100000)\n",
    "#pprint(agent.q)\n",
    "\n",
    "#test\n",
    "agent.config['eps'] = 0 #all greedy o/\n",
    "env.print_deterministic_policy(agent.greedy_policy())\n",
    "#run(agent, env, render=True, verbose=True)\n",
    "run(agent, env, render=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym_adversarialgrid.agents.tabular as tabular\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "def categorical_draw(probabilities):\n",
    "    \"\"\"\n",
    "    Selects an option with a roulette-like process\n",
    "    :param probabilities:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    z = random.random()\n",
    "    cum_prob = 0.0\n",
    "\n",
    "    for choice, prob in enumerate(probabilities):\n",
    "        cum_prob += prob\n",
    "        if cum_prob > z:\n",
    "            return choice\n",
    "\n",
    "    print('Warning: categorical_draw reached its end')\n",
    "    return len(probabilities) - 1  # I think code should not reach here\n",
    "\n",
    "\n",
    "class SGExp3(tabular.TabularQAgent):\n",
    "    \"\"\"\n",
    "    Extends Exp3 (Auer et. al 1995) with the notion of state.\n",
    "    The implementation of Exp3 we are extending is the one shown in Auer et. al 2002.\n",
    "\n",
    "    References:\n",
    "\n",
    "    Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (1995).\n",
    "    Gambling in a rigged casino: The adversarial multi-armed bandit problem.\n",
    "    Proceedings of IEEE 36th Annual Foundations of Computer Science, 322–331.\n",
    "    https://doi.org/10.1109/SFCS.1995.492488\n",
    "\n",
    "    Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (2002).\n",
    "    The nonstochastic multiarmed bandit problem.\n",
    "    Society for Industrial and Applied Mathematics, 32(1), 48–77.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SGExp3, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.gamma = kwargs['gamma'] if 'gamma' in kwargs else 0.2\n",
    "        self.discount = kwargs['discount'] if 'discount' in kwargs else 0.9\n",
    "\n",
    "        n_actions = self.action_space.n\n",
    "        \n",
    "        # cannot initialize q with zeroes\n",
    "        self.q = defaultdict(\n",
    "            lambda: [0.01] * n_actions\n",
    "        )\n",
    "        \n",
    "        # policy initialized as uniformly random\n",
    "        self.policy = defaultdict(lambda: [1.0 / n_actions] * n_actions)\n",
    "\n",
    "    def calculate_policy(self, state):\n",
    "        \"\"\"\n",
    "        Calculates the policy for a given state and returns it\n",
    "        :param state: \n",
    "        :return: list(float) the policy (probability vector) for that state\n",
    "        \"\"\"\n",
    "        # short aliases\n",
    "        s = state  # s stands for state\n",
    "        g = self.gamma  # g stands for gamma\n",
    "        n = self.action_space.n  # n stands for the number of actions\n",
    "        pi_s = self.policy[state]  # pi_s stands for the policy in state s\n",
    "\n",
    "        sum_weights = sum(self.q[s])\n",
    "\n",
    "        # the policy is a probability vector, giving the probability of each action\n",
    "        # pi(s, . ) = [(1 - gamma)*q(s,a) + gamma / n] - for each action\n",
    "        #print(state, pi_s, self.q[s])\n",
    "        pi_s = [((1 - g) * value / sum_weights) + (g / n) for value in self.q[s]]\n",
    "        #print(state, pi_s)\n",
    "        return pi_s\n",
    "\n",
    "    def act(self, observation):\n",
    "        prob_vector = self.calculate_policy(observation)\n",
    "        return categorical_draw(prob_vector)\n",
    "\n",
    "    def learn(self, s, a, reward, sprime, done):\n",
    "        # aliases:\n",
    "        pi_sp = self.policy[sprime]  # the policy for the next state\n",
    "        q_sp = self.q[sprime]  # the action values for next state\n",
    "        n = self.action_space.n  # the number of actions\n",
    "\n",
    "        # value of next state, it is zero if current state is terminal\n",
    "        future = sum([pi_sp[ap] * value for ap, value in enumerate(q_sp)]) if not done else 0\n",
    "\n",
    "        # x is a value to be scaled and weighted by its probability\n",
    "        x = reward + self.discount * future\n",
    "\n",
    "        # scales x to [0, 1] - assuming minimum reward is -1 and max reward is +1\n",
    "        # rescaling as per https://en.wikipedia.org/wiki/Feature_scaling#Rescaling\n",
    "        max_x = 1 + self.discount\n",
    "        min_x = -1 - self.discount\n",
    "\n",
    "        scaled_x = (x - min_x) / (max_x - min_x)\n",
    "\n",
    "        # weights the value by its probability\n",
    "        x_hat = scaled_x / self.policy[s][a]\n",
    "\n",
    "        # finally updates the value\n",
    "        self.q[s][a] = self.q[s][a] * math.exp(self.gamma * x_hat / n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SG-Exp3 q vs Fixed-NOOP\n",
    "import gym_adversarialgrid.envs.adversarialgrid as adversarialgrid\n",
    "\n",
    "env = adversarialgrid.AdversarialGrid(opponent='Fixed', map='3x4', action=adversarialgrid.NOOP)\n",
    "agent = SGExp3(env.observation_space, env.action_space, gamma=0.2)\n",
    "\n",
    "#train\n",
    "agent.train(env, 1000)\n",
    "#pprint(agent.q)\n",
    "\n",
    "#test\n",
    "agent.config['eps'] = 0 #all greedy o/\n",
    "#env.print_deterministic_policy(agent.greedy_policy())\n",
    "#run(agent, env, render=True, verbose=True)\n",
    "run(agent, env, render=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
