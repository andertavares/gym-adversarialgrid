{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run(agent, env, episodes=20, episode_duration=100, render=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs a reinforcement learning experiment\n",
    "    :param agent: implements the method act(observation)\n",
    "    :param env: an instance of gym.Env\n",
    "    :param episodes: number of episodes to run\n",
    "    :param episode_duration: number of steps of each episode\n",
    "    :param render: render each step?\n",
    "    :param verbose: print transition information?\n",
    "    \"\"\"\n",
    "    for ep in range(episodes):\n",
    "        observation = env.reset()\n",
    "        acc_reward = 0\n",
    "        if render:\n",
    "            env.render() # show initial state\n",
    "        for t in range(100):         \n",
    "            # print(observation)\n",
    "            action = agent.act(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            acc_reward += reward\n",
    "            if verbose:\n",
    "                print(observation, reward, done, info)          \n",
    "            if render:\n",
    "                env.render() \n",
    "                \n",
    "            if done:\n",
    "                print(\"Episode {} finished after {} timesteps w/ total reward {}\".format(ep+1, t+1, acc_reward))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test actual agents in actual environments\n",
    "import gym_adversarialgrid.envs.adversarialgrid as adversarialgrid\n",
    "import gym_adversarialgrid.agents.tabular as tabular\n",
    "import gym_adversarialgrid.agents.exp3mg as exp3mg\n",
    "import gym_adversarialgrid.agents.hedgemg as hedgemg\n",
    "import gym_adversarialgrid.agents.minimaxq as minimaxq\n",
    "\n",
    "num_iter = 1000 #number of iterations\n",
    "#g = min(1, math.sqrt( (num_iter * math.log(num_iter)) / ((math.e - 1) * num_iter) ))\n",
    "g = 0.1 # for hedge and Exp3\n",
    "\n",
    "#env = adversarialgrid.AdversarialGrid(opponent='Fixed', map='3x4', action=adversarialgrid.NOOP)\n",
    "env = adversarialgrid.AdversarialGrid(opponent='Random', map='3x4') #, action=adversarialgrid.NOOP)\n",
    "\n",
    "#agent = tabular.TabularQAgent(env.observation_space, env.action_space, eps=0.1, init_mean=1)\n",
    "#agent = exp3mg.Exp3MG(env.observation_space, env.action_space, gamma=g)\n",
    "#agent = exp3mg.Exp3MG_1995(env.observation_space, env.action_space, gamma=g)\n",
    "#agent = hedgemg.HedgeMG(env.observation_space, env.action_space, gamma=g)\n",
    "#agent = hedgemg.HedgeMG_1995(env.observation_space, env.action_space, gamma=g)\n",
    "agent = minimaxq.MinimaxQ(env.observation_space, env.action_space, env.opp_action_space, gamma=g)\n",
    "\n",
    "#train\n",
    "agent.train(env, num_iter)\n",
    "#pprint(agent.q)\n",
    "\n",
    "#test\n",
    "agent.config['eps'] = 0 #all greedy o/\n",
    "env.print_deterministic_policy(agent.greedy_policy())\n",
    "#run(agent, env, render=True, verbose=True)\n",
    "run(agent, env, render=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Exp3(tabular.TabularQAgent):\n",
    "    \"\"\"\n",
    "    Auer 2002 implementation of the Exp3 method\n",
    "\n",
    "    References:\n",
    "\n",
    "    Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (2002).\n",
    "    The nonstochastic multiarmed bandit problem.\n",
    "    Society for Industrial and Applied Mathematics, 32(1), 48–77.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Exp3, self).__init__(*args, **kwargs)\n",
    "        self.gamma = kwargs['gamma'] if 'gamma' in kwargs else 0.07\n",
    "        self.weights = [1.0] * self.action_space.n\n",
    "        \n",
    "    def policy(weights, gamma=0.0):\n",
    "        the_sum = float(sum(weights))\n",
    "        return tuple((1.0 - gamma) * (w / the_sum) + (gamma / len(weights)) for w in weights)\n",
    " \n",
    "    def act(self, observation):\n",
    "        the_policy = Exp3.policy(self.weights, self.gamma)\n",
    "        return categorical_draw(the_policy)\n",
    "        \n",
    "    def learn(self, s, a, reward, sprime, done):\n",
    "        policy = Exp3.policy(self.weights, self.gamma)\n",
    "        estimatedReward = 1.0 * reward / policy[a]\n",
    "        self.weights[a] *= math.exp(estimatedReward * self.gamma / self.action_space.n) # important that we use estimated reward here!\n",
    "        #print(['%.3f' % w for w in self.weights])\n",
    "        #print(['%.3f' % w for w in Exp3.policy(self.weights)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "from six import StringIO, b\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "class Bandit(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, n_arms=10):\n",
    "        self.bandits_mu = stats.norm(0, 1).rvs(n_arms)\n",
    "        self.sigma = 1\n",
    "        self.last_choice = -1\n",
    "        self.last_reward = -10\n",
    "        \n",
    "        #bernoulli arm:\n",
    "        #self.reward_vector = [[1 if random.random() < bias else 0 for bias in biases] for _ in range(numRounds)]\n",
    "        self.arms = [stats.bernoulli(1 / (k+2) ) for k in range(n_arms)]\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(n_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "        #self.arms = [stats.norm(self.bandits_mu[arm], self.sigma) for arm in n_arms]\n",
    "\n",
    "    def _step(self, action):\n",
    "        self.last_choice = action\n",
    "        reward = self.arms[action].rvs(1) #np.random.normal(self.bandits_mu[action], self.sigma)\n",
    "        self.last_reward = reward\n",
    "        info = {\n",
    "            \"action\": action,\n",
    "            \"expected_rwd\": self.arms[action].mean()\n",
    "        }\n",
    "        \n",
    "        # return: state, reward, done, info\n",
    "        return 0, reward, True, info\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.last_choice = -1\n",
    "        self.last_reward = -10\n",
    "        \n",
    "    def _render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            return\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        #row, col = self.current_state  # self.s // self.ncols, self.s % self.ncols\n",
    "        desc = ' '.join(['%.3f' % mu for mu in self.bandits_mu])\n",
    "        #desc = [c.decode('utf-8') for c in desc] \n",
    "        #desc[col] = utils.colorize(desc[col], \"red\", highlight=True)\n",
    "        if self.last_choice is not None:\n",
    "            outfile.write(\"  ({}, {})\\n\".format(self.last_choice, self.last_reward))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(desc + \"\\n\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            return outfile\n",
    "        \n",
    "# test the exp3 agent here\n",
    "env = Bandit(n_arms=10)\n",
    "#agent = SGExp3(env.observation_space, env.action_space, gamma=0.2)\n",
    "agent = Exp3(env.observation_space, env.action_space, gamma=0.2)\n",
    "\n",
    "#train\n",
    "agent.train(env, 1000)\n",
    "\n",
    "#test\n",
    "run(agent, env, render=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# C-Exp3: contextual Exp3 -- doesn't work well because it can't discover sequential actions \n",
    "# what if we use MC returns?\n",
    "import gym_adversarialgrid.agents.tabular as tabular\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class CExp3(tabular.TabularQAgent):\n",
    "    \"\"\"\n",
    "    Contextual Exp3: Exp3 (Auer et. al 1995) for contextual bandits\n",
    "    The implementation of Exp3 we are extending is the one shown in Auer et. al 2002.\n",
    "\n",
    "    References:\n",
    "\n",
    "    Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (1995).\n",
    "    Gambling in a rigged casino: The adversarial multi-armed bandit problem.\n",
    "    Proceedings of IEEE 36th Annual Foundations of Computer Science, 322–331.\n",
    "    https://doi.org/10.1109/SFCS.1995.492488\n",
    "\n",
    "    Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (2002).\n",
    "    The nonstochastic multiarmed bandit problem.\n",
    "    Society for Industrial and Applied Mathematics, 32(1), 48–77.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CExp3, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.gamma = kwargs['gamma'] if 'gamma' in kwargs else 0.2\n",
    "\n",
    "        n_actions = self.action_space.n\n",
    "        \n",
    "        # cannot initialize q with zeroes\n",
    "        self.q = defaultdict(\n",
    "            #lambda: [0.01] * n_actions\n",
    "            lambda: [1.] * n_actions\n",
    "        )\n",
    "        \n",
    "        # policy initialized as uniformly random\n",
    "        self.policy = defaultdict(lambda: [1.0 / n_actions] * n_actions)\n",
    "\n",
    "    def calculate_policy(self, state):\n",
    "        \"\"\"\n",
    "        Calculates the policy for a given state and returns it\n",
    "        :param state: \n",
    "        :return: list(float) the policy (probability vector) for that state\n",
    "        \"\"\"\n",
    "        # short aliases\n",
    "        s = state  # s stands for state\n",
    "        g = self.gamma  # g stands for gamma\n",
    "        n = self.action_space.n  # n stands for the number of actions\n",
    "        pi_s = self.policy[state]  # pi_s stands for the policy in state s\n",
    "\n",
    "        sum_weights = sum(self.q[s])\n",
    "\n",
    "        # the policy is a probability vector, giving the probability of each action\n",
    "        # pi(s, . ) = [(1 - gamma)*q(s,a) + gamma / n] - for each action\n",
    "        #print(state, pi_s, self.q[s])\n",
    "        pi_s = [((1 - g) * value / sum_weights) + (g / n) for value in self.q[s]]\n",
    "        #print(state, pi_s)\n",
    "        return pi_s\n",
    "\n",
    "    def act(self, observation):\n",
    "        prob_vector = self.calculate_policy(observation)\n",
    "        return categorical_draw(prob_vector)\n",
    "\n",
    "    def learn(self, s, a, reward, sprime, done):\n",
    "        # aliases:\n",
    "        #pi_sp = self.policy[sprime]  # the policy for the next state\n",
    "        #q_sp = self.q[sprime]  # the action values for next state\n",
    "        n = self.action_space.n  # the number of actions\n",
    "\n",
    "        # x is a value to be scaled and weighted by its probability\n",
    "        x = reward\n",
    "\n",
    "        # scales x to [0, 1] - assuming minimum reward is -1 and max reward is +1\n",
    "        # rescaling as per https://en.wikipedia.org/wiki/Feature_scaling#Rescaling\n",
    "        max_x = 1 \n",
    "        min_x = -1 \n",
    "\n",
    "        scaled_x = (x - min_x) / (max_x - min_x)\n",
    "\n",
    "        # weights the value by its probability\n",
    "        x_hat = scaled_x / self.policy[s][a]\n",
    "\n",
    "        # finally updates the value\n",
    "        print('q(s,a), r, x, ~x, ^x = %.3f, %.3f, %.3f, %.3f, %.3f' % (self.q[s][a], reward, x, scaled_x, x_hat))\n",
    "        self.q[s][a] *= math.exp(self.gamma * x_hat / n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# C-Exp3 q vs Fixed-NOOP -- seems that C-Exp3 does not work \n",
    "# because rewards are sparse and agent cannot 'connect' action sequences\n",
    "import gym_adversarialgrid.envs.adversarialgrid as adversarialgrid\n",
    "\n",
    "env = adversarialgrid.AdversarialGrid(opponent='Fixed', map='3x4', action=adversarialgrid.NOOP)\n",
    "agent = CExp3(env.observation_space, env.action_space, gamma=0.2)\n",
    "\n",
    "#train\n",
    "agent.train(env, 10000)\n",
    "#pprint(agent.q)\n",
    "\n",
    "#test\n",
    "agent.config['eps'] = 0 #all greedy o/\n",
    "#env.print_deterministic_policy(agent.greedy_policy())\n",
    "#run(agent, env, render=True, verbose=True)\n",
    "run(agent, env, render=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array([[2,1], [3, 4], [5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
