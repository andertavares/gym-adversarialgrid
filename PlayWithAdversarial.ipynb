{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_adversarialgrid.envs.adversarialgrid import AdversarialGrid\n",
    "\n",
    "env = AdversarialGrid()\n",
    "\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()  # random action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(observation, reward, done, info)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# A tabular Q-learning agent\n",
    "import gym\n",
    "import gym.spaces.discrete as discrete\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "#print(discrete.Discrete)\n",
    "\n",
    "class TabularQAgent(object):\n",
    "    \"\"\"\n",
    "    Agent implementing tabular Q-learning.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, **userconfig):\n",
    "        if not isinstance(observation_space, discrete.Discrete):\n",
    "            raise UnsupportedSpace('Observation space {} incompatible with {}. (Only supports Discrete observation spaces.)'.format(observation_space, self))\n",
    "        if not isinstance(action_space, discrete.Discrete):\n",
    "            raise UnsupportedSpace('Action space {} incompatible with {}. (Only supports Discrete action spaces.)'.format(action_space, self))\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.action_n = action_space.n\n",
    "        self.config = {\n",
    "            \"init_mean\" : 0.0,      # Initialize Q values with this mean\n",
    "            \"init_std\" : 0.0,       # Initialize Q values with this standard deviation\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"eps\": 0.05,            # Epsilon in epsilon greedy policies\n",
    "            \"discount\": 0.95,\n",
    "            \"n_iter\": 10000}        # Number of iterations\n",
    "        self.config.update(userconfig)\n",
    "        self.q = defaultdict(lambda: self.config[\"init_std\"] * np.random.randn(self.action_n) + self.config[\"init_mean\"])\n",
    "        \n",
    "    def act(self, observation, eps=None):\n",
    "        \"\"\"\n",
    "        Selects an action via epsilon greedy \n",
    "        :param observation: current 'state'\n",
    "        :param eps: epsilon\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if eps is None:\n",
    "            eps = self.config[\"eps\"]\n",
    "        \n",
    "        # epsilon greedy\n",
    "        action = self.action_space.sample()\n",
    "        if np.random.random() > eps:\n",
    "            # an argmax that breaks ties randomly: https://stackoverflow.com/a/42071648\n",
    "            b = self.q[observation]\n",
    "            action = np.random.choice(np.flatnonzero(b == b.max())) \n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self, s, a, reward, sprime, done):\n",
    "        \"\"\"\n",
    "        Updates Q of previous action and observation\n",
    "        :param s: current observation\n",
    "        :param a: action taken\n",
    "        :param reward: the reward signal\n",
    "        :param sprime: observation after taking a in s\n",
    "        :param done: is current state terminal?\n",
    "        \n",
    "        \"\"\"\n",
    "        q = self.q  # alias\n",
    "\n",
    "        # value of 'future' state\n",
    "        future = np.max(self.q[sprime]) if not done else 0.0\n",
    "\n",
    "        # applies update rule: Q(s,a) = Q(s,a) + alpha(r + gamma* max_{a'}Q(s',a') - Q(s,a))\n",
    "        newq = q[s][a] + self.config[\"learning_rate\"] * (reward + self.config[\"discount\"] * future - q[s][a])\n",
    "        q[s][a] = newq\n",
    "        \n",
    "    def train(self, env):\n",
    "        config = self.config\n",
    "        observation = env.reset()\n",
    "        for t in range(config[\"n_iter\"]):\n",
    "            action = self.act(observation)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            self.learn(observation, action, reward, next_obs, done)\n",
    "            observation = next_obs if not done else env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# testing the tabular q agent:\n",
    "import gym\n",
    "from pprint import pprint\n",
    "from adversarialgrid import AdversarialGrid\n",
    "\n",
    "env = AdversarialGrid()\n",
    "agent = TabularQAgent(env.observation_space, env.action_space, eps=0.1, init_mean=1)\n",
    "\n",
    "#train\n",
    "agent.train(env)\n",
    "#pprint(agent.q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "agent.config['eps'] = 0 #all greedy o/\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = agent.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(observation, reward, done, info)\n",
    "        if done:\n",
    "            env.render()\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
