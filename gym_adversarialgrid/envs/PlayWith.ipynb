{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from adversarialgrid import AdversarialGrid\n",
    "\n",
    "env = AdversarialGrid()\n",
    "\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(observation, reward, done, info)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A tabular Q-learning agent\n",
    "import gym\n",
    "import gym.spaces.discrete as discrete\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "#print(discrete.Discrete)\n",
    "\n",
    "class TabularQAgent(object):\n",
    "    \"\"\"\n",
    "    Agent implementing tabular Q-learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, **userconfig):\n",
    "        if not isinstance(observation_space, discrete.Discrete):\n",
    "            raise UnsupportedSpace('Observation space {} incompatible with {}. (Only supports Discrete observation spaces.)'.format(observation_space, self))\n",
    "        if not isinstance(action_space, discrete.Discrete):\n",
    "            raise UnsupportedSpace('Action space {} incompatible with {}. (Only supports Discrete action spaces.)'.format(action_space, self))\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.action_n = action_space.n\n",
    "        self.config = {\n",
    "            \"init_mean\" : 0.0,      # Initialize Q values with this mean\n",
    "            \"init_std\" : 0.0,       # Initialize Q values with this standard deviation\n",
    "            \"learning_rate\" : 0.1,\n",
    "            \"eps\": 0.05,            # Epsilon in epsilon greedy policies\n",
    "            \"discount\": 0.95,\n",
    "            \"n_iter\": 10000}        # Number of iterations\n",
    "        self.config.update(userconfig)\n",
    "        self.q = defaultdict(lambda: self.config[\"init_std\"] * np.random.randn(self.action_n) + self.config[\"init_mean\"])\n",
    "\n",
    "    def act(self, observation, eps=None):\n",
    "        if eps is None:\n",
    "            eps = self.config[\"eps\"]\n",
    "        # epsilon greedy.\n",
    "        action = np.argmax(self.q[observation]) if np.random.random() > eps else self.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def learn(self, env):\n",
    "        config = self.config\n",
    "        obs = env.reset()\n",
    "        q = self.q\n",
    "        for t in range(config[\"n_iter\"]):\n",
    "            action = self.act(obs)\n",
    "            obs2, reward, done, _ = env.step(action)\n",
    "            future = 0.0\n",
    "            if not done:\n",
    "                future = np.max(q[obs2])\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"s, a, r, s': {}, {}, {}, {}\".format(obs, action, reward, obs2))\n",
    "            \n",
    "            #Q(s,a) = Q(s,a) + alpha(r + gamma* max_{a'}Q(s',a') - Q(s,a))\n",
    "            newq = q[obs][action] + self.config[\"learning_rate\"] * (reward + config[\"discount\"] * future - q[obs][action])\n",
    "            print(\"Q(s,a) <- {}\".format(newq))\n",
    "            \n",
    "            q[obs][action] = newq\n",
    "            \n",
    "            #q[obs][action] += self.config[\"learning_rate\"] * (reward + config[\"discount\"] * future - q[obs][action])\n",
    "            #q[obs][action] -= \\\n",
    "            #    self.config[\"learning_rate\"] * (q[obs][action] - reward - config[\"discount\"] * future)\n",
    "\n",
    "            obs = obs2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing the tabular q agent:\n",
    "import gym\n",
    "from pprint import pprint\n",
    "from adversarialgrid import AdversarialGrid\n",
    "\n",
    "env = AdversarialGrid()\n",
    "agent = TabularQAgent(env.observation_space, env.action_space, eps=0.9, init_mean=1)\n",
    "\n",
    "#train\n",
    "agent.learn(env)\n",
    "pprint(agent.q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = agent.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(observation, reward, done, info)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
